{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the test data filepath\n",
    "s3_path = os.path.join('test_data', 'allplayers.json')\n",
    "jsonfile = open(s3_path)\n",
    "contents = json.load(jsonfile)\n",
    "# Convert JSON to dataframe\n",
    "allplayers = pd.DataFrame.from_dict(contents)\n",
    "# Transpose dataframe\n",
    "allplayers = allplayers.T\n",
    "# Create a column for id_sleeper based on the index\n",
    "allplayers = allplayers.reset_index(names='id_sleeper')\n",
    "# Select only relevant columns\n",
    "allplayers = allplayers[[\n",
    "    'id_sleeper',\n",
    "    'full_name', \n",
    "    'weight', 'height',\n",
    "    'birth_date', 'age', \n",
    "    'high_school', 'college',\n",
    "    'sport', 'years_exp', 'active', 'status',\n",
    "    'team', 'number', 'position', 'fantasy_positions', 'depth_chart_position', 'depth_chart_order',\n",
    "    'news_updated', 'injury_status', 'injury_body_part', 'injury_start_date', 'injury_notes', 'practice_description', 'practice_participation',\n",
    "    'competitions',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read static data\n",
    "ids = pd.read_csv('test_data/lu_ids.csv')\n",
    "# Read other scraped data\n",
    "ourlads = pd.read_csv('test_data/ourlads.csv')\n",
    "sharks = pd.read_csv('test_data/sharks.csv', dtype={'Week':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks['id_sharks'] = sharks['id_sharks'].apply(lambda x: ' '.join(x.split(', ')[::-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scoring dataset\n",
    "scoring = ids.merge(\n",
    "    sharks, how='inner', on='id_sharks'\n",
    ").merge(\n",
    "    ourlads, how = 'left', on='id_ourlads'\n",
    ")\n",
    "# Clean column names\n",
    "scoring = scoring.drop(columns=[\n",
    "    'id_ourlads', 'id_sharks', '#', 'Tm', 'Opp',\n",
    "])\n",
    "\n",
    "# Create a primary key\n",
    "scoring['index_scoring'] = scoring['id_sleeper'] + \"_\" + scoring['Week']\n",
    "# Drop NA values\n",
    "scoring = scoring.dropna(subset='index_scoring')\n",
    "# Drop duplicates\n",
    "scoring = scoring.drop_duplicates(subset='index_scoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive additional columns from others\n",
    "scoring.loc[scoring['PR']==True, 'pr_yd'] = 13\n",
    "scoring.loc[scoring['KR']==True, 'kr_yd'] = 19\n",
    "scoring['fgm_yds_over_30'] = scoring['30-39 FGM'] + scoring['40-49 FGM'] + scoring['50+ FGM']\n",
    "scoring.loc[(scoring['Pts Agn']<1), 'pts_allow_0'] = 1\n",
    "scoring.loc[(scoring['Pts Agn']>=1) & (scoring['Pts Agn']<7), 'pts_allow_1_6'] = 1\n",
    "scoring.loc[(scoring['Pts Agn']>=7) & (scoring['Pts Agn']<14), 'pts_allow_7_13'] = 1\n",
    "scoring.loc[(scoring['Pts Agn']>=14) & (scoring['Pts Agn']<21), 'pts_allow_14_20'] = 1\n",
    "scoring.loc[(scoring['Pts Agn']>=21) & (scoring['Pts Agn']<28), 'pts_allow_21_27'] = 1\n",
    "scoring.loc[(scoring['Pts Agn']>=28) & (scoring['Pts Agn']<35), 'pts_allow_28_34'] = 1\n",
    "scoring.loc[(scoring['Pts Agn']>=35), 'pts_allow_35p'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']<100), 'yds_allow_0_100'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']>=100) & (scoring['Yds Allowed']<200), 'yds_allow_100_199'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']>=200) & (scoring['Yds Allowed']<300), 'yds_allow_200_299'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']>=400) & (scoring['Yds Allowed']<450), 'yds_allow_400_449'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']>=450) & (scoring['Yds Allowed']<500), 'yds_allow_450_499'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']>=500) & (scoring['Yds Allowed']<550), 'yds_allow_500_549'] = 1\n",
    "scoring.loc[(scoring['Yds Allowed']>=550), 'yds_allow_550p'] = 1\n",
    "scoring.loc[(scoring['Rsh Yds']>=100) & (scoring['Rsh Yds']<200), 'bonus_rush_yd_100'] = 1\n",
    "scoring.loc[scoring['Rsh Yds']>=200, 'bonus_rush_yd_200'] = 1\n",
    "scoring.loc[(scoring['Rec Yds']>=100) & (scoring['Rec Yds']<200), 'bonus_rec_yd_100'] = 1\n",
    "scoring.loc[scoring['Rec Yds']>=200, 'bonus_rec_yd_200'] = 1\n",
    "scoring.loc[(scoring['Pass Yds']>=300) & (scoring['Pass Yds']<400), 'bonus_pass_yd_300'] = 1\n",
    "scoring.loc[scoring['Pass Yds']>=400, 'bonus_pass_yd_400'] = 1\n",
    "scoring['Rush and Rec Yds'] = scoring['Rsh Yds'] + scoring['Rec Yds']\n",
    "scoring.loc[scoring['Rush and Rec Yds']>=200, 'bonus_rush_rec_yd_200'] = 1\n",
    "\n",
    "# Rename columns\n",
    "scoring = scoring.rename(columns={\n",
    "    'Week':'week_of_season',\n",
    "    'Comp':'pass_cmp', 'Pass Yds':'pass_yd', 'Pass TDs':'pass_td', \n",
    "    'Int':'pass_int', \n",
    "    'Rush':'rush_att', 'Rsh Yds':'rush_yd', 'Rsh TDs':'rush_td', \n",
    "    'Fum':'fum_lost',\n",
    "    'Rec':'rec','Rec Yds':'rec_yd', 'Rec TDs':'rec_td', \n",
    "    'XPM':'xpm', 'FGM':'fgm', '10-19 FGM':'fgm_0_19','20-29 FGM':'fgm_20_29', '30-39 FGM':'fgm_30_39', '40-49 FGM':'fgm_40_49', '50+ FGM':'fgm_50p',\n",
    "    'Miss':'fgmiss', \n",
    "    'Scks':'sack', 'DefTD':'def_st_td', 'Safts':'safe',     \n",
    "})\n",
    "# Drop unnecessary columns which sharks has but sleeper lacks\n",
    "scoring = scoring.drop(columns=[\n",
    "    'Att', '0-9 Pass TDs', '10-19 Pass TDs', '20-29 Pass TDs', '30-39 Pass TDs', '40-49 Pass TDs', '50+ Pass TDs', 'Sck', \n",
    "    '0-9 Rsh TDs', '10-19 Rsh TDs', '20-29 Rsh TDs', '30-39 Rsh TDs', '40-49 Rsh TDs', '50+ Rsh TDs',\n",
    "    '>= 50 yd', '>= 100 yd','0-9 Rec TDs', '10-19 Rec TDs', '20-29 Rec TDs', '30-39 Rec TDs','40-49 Rec TDs', '50+ Rec TDs',\n",
    "    'Tgt', 'RZ Tgt', \n",
    "    'Kick Ret Yds','PR', 'KR',\n",
    "    'XPA','FGA', \n",
    "    'Punts','Punt Yds', 'Punts Inside 20', \n",
    "    'Yds Allowed', 'Pts Agn', \n",
    "])\n",
    "# Set to zero predictions which sharks lacks but sleeper has\n",
    "for colName in [\n",
    "    'pass_2pt', 'rush_2pt', 'rec_2pt', 'xpmiss', \n",
    "    'int', 'fum_rec', 'blk_kick', 'ff','def_st_ff','def_st_fum_rec',\n",
    "    'def_td','def_3_and_out','def_2pt',\n",
    "    'st_fum_rec','st_ff','st_td',\n",
    "    'fum','fum_rec_td',\n",
    "]:\n",
    "    scoring[colName] = 0\n",
    "    # Change each column to float64 data type\n",
    "    scoring[colName] = scoring[colName].astype('float64')\n",
    "# Set NA values to zero\n",
    "scoring = scoring.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy up columns\n",
    "scoring = scoring[[\n",
    "    'index_scoring', 'id_sleeper', 'week_of_season', 'pass_cmp', 'pass_yd',\n",
    "    'pass_td', 'pass_int', 'rush_att', 'rush_yd', 'rush_td', 'fum_lost',\n",
    "    'rec', 'rec_yd', 'rec_td', 'xpm', 'fgm', 'fgm_0_19', 'fgm_20_29',\n",
    "    'fgm_30_39', 'fgm_40_49', 'fgm_50p', 'fgmiss', 'sack', 'def_st_td',\n",
    "    'safe', 'pr_yd', 'kr_yd', 'fgm_yds_over_30', 'pts_allow_0',\n",
    "    'pts_allow_1_6', 'pts_allow_7_13', 'pts_allow_14_20', 'pts_allow_21_27',\n",
    "    'pts_allow_28_34', 'pts_allow_35p', 'yds_allow_0_100',\n",
    "    'yds_allow_100_199', 'yds_allow_200_299', 'yds_allow_400_449',\n",
    "    'yds_allow_450_499', 'yds_allow_500_549', 'yds_allow_550p',\n",
    "    'bonus_rush_yd_100', 'bonus_rush_yd_200', 'bonus_rec_yd_100',\n",
    "    'bonus_rec_yd_200', 'bonus_pass_yd_300', 'bonus_pass_yd_400',\n",
    "    'Rush and Rec Yds', 'bonus_rush_rec_yd_200', 'pass_2pt', 'rush_2pt',\n",
    "    'rec_2pt', 'xpmiss', 'int', 'fum_rec', 'blk_kick', 'ff', 'def_st_ff',\n",
    "    'def_st_fum_rec', 'def_td', 'def_3_and_out', 'def_2pt', 'st_fum_rec',\n",
    "    'st_ff', 'st_td', 'fum', 'fum_rec_td'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 ('fnfl2024')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33dbe7c72db16b281083c7de59ed8ec8f22d2824957f89e4bfb828dc6fc75088"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
